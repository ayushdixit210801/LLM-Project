# PDF Question Answering System with RAG

A Retrieval-Augmented Generation (RAG) system that allows users to upload PDF documents and ask questions about their content. The system uses state-of-the-art language models and vector search to provide accurate, context-aware answers.

## Features

- 📄 PDF document upload and processing
- 🔍 Semantic search using vector embeddings
- 💡 Intelligent question answering using Google's Gemini model
- 📚 Document management with metadata tracking
- 🚀 Fast API with interactive Swagger UI documentation

## Architecture

The system uses a modern tech stack:

- **Backend Framework**: FastAPI
- **Vector Embeddings**: Sentence Transformers (all-MiniLM-L6-v2)
- **Vector Storage**: FAISS (Facebook AI Similarity Search) for efficient similarity search
- **Text Generation**: Google Gemini 2.0 Flash
- **PDF Processing**: PyPDF2 for extraction, NLTK for text chunking
- **API Documentation**: Swagger UI (automatically generated by FastAPI)

## Performance Features

- **Efficient Vector Search**: Uses FAISS for fast similarity search with millions of vectors
- **Normalized Vectors**: Implements cosine similarity through L2 normalization and inner product
- **Persistent Storage**: Automatically saves and loads index state
- **Document Filtering**: Efficient filtering by document ID during search

## Setup

### Prerequisites

- Python 3.11+
- pip package manager
- Google Cloud API key for Gemini access

### Local Setup

1. Clone the repository:
```bash
git clone https://github.com/ayushdixit210801/LLM-Project.git
cd LLM-Project
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up environment variables:
```bash
# Create .env file
echo "GEMINI_API_KEY=your_api_key_here" > .env
```

5. Run the application:
```bash
uvicorn backend.main:app --reload
```

### Docker Setup

1. Build the Docker image:
```bash
docker build -t llm-project .
```

2. Run the container:
```bash
docker run -p 8000:8000 --name llm-container llm-project:latest
```

## API Usage

The API is available at `http://localhost:8000` with interactive documentation at `http://localhost:8000/docs`.

### Endpoints

#### 1. Upload PDF
- **Endpoint**: POST `/upload`
- **Purpose**: Upload and process a PDF document
- **Request**: Multipart form with PDF file
- **Response**: Document metadata including ID
```json
{
    "message": "PDF processed successfully",
    "document": {
        "id": "doc_id",
        "filename": "example.pdf",
        "total_pages": 10,
        "total_chunks": 25,
        "file_size": 1024
    }
}
```

#### 2. Query Document
- **Endpoint**: POST `/query`
- **Purpose**: Ask questions about a specific document
- **Request**:
```json
{
    "docId": "document_id",
    "query": "What is the main topic of chapter 3?"
}
```
- **Response**:
```json
{
    "answer": "Based on the document...",
    "sources": ["Page 12", "Page 13"]
}
```

#### 3. List Documents
- **Endpoint**: GET `/documents`
- **Purpose**: List all processed documents
- **Parameters**: 
  - `skip` (optional): Number of documents to skip
  - `limit` (optional): Maximum number of documents to return
- **Response**: List of document metadata

#### 4. Get Document
- **Endpoint**: GET `/documents/{document_id}`
- **Purpose**: Get metadata for a specific document
- **Response**: Document metadata

## How It Works

1. **Document Processing**:
   - PDF is uploaded and text is extracted
   - Text is split into chunks with overlap
   - Each chunk is embedded using Sentence Transformers
   - Embeddings and metadata are stored in the vector store

2. **Question Answering**:
   - User query is received with document ID
   - Query is embedded and similar chunks are retrieved
   - Retrieved context is sent to Gemini model
   - Model generates a natural language answer

3. **Vector Search**:
   - Uses cosine similarity for semantic matching
   - Filters results by document ID
   - Returns most relevant text chunks with metadata

## Libraries Used

- **FastAPI**: Modern web framework for building APIs
- **Sentence Transformers**: For generating text embeddings
- **google.generativeai**: Google's Gemini model for text generation
- **PyPDF2**: PDF text extraction
- **NLTK**: Text processing and chunking
- **SQLAlchemy**: Database ORM for document metadata
- **Pydantic**: Data validation and settings management
- **uvicorn**: ASGI server for running the application

## Development

The system is designed to be modular and extensible. Key components:

- `backend/routes/endpoints.py`: API route definitions
- `backend/services/document_ingestion.py`: PDF processing pipeline
- `backend/services/rag_pipeline.py`: Question answering logic
- `backend/services/vector_store.py`: Vector storage and search
- `backend/models/`: Database models and schemas
